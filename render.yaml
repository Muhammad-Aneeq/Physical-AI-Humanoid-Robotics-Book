# Render Blueprint for RAG Chatbot Backend
# Deploy with: Connect GitHub repo to Render and it auto-detects this file

services:
  # FastAPI Backend
  - type: web
    name: rag-chatbot-api
    runtime: python
    env: python
    region: oregon  # or your preferred region
    plan: free  # or starter/standard
    branch: main  # or your deployment branch
    rootDir: backend
    buildCommand: pip install -r requirements.txt
    startCommand: uvicorn main:app --host 0.0.0.0 --port $PORT
    healthCheckPath: /api/health
    envVars:
      # OpenAI Configuration
      - key: OPENAI_API_KEY
        sync: false  # Set manually in Render dashboard
      - key: OPENAI_CHAT_MODEL
        value: gpt-4o-mini
      - key: OPENAI_TEMPERATURE
        value: 0.3
      - key: OPENAI_MAX_TOKENS
        value: 1000

      # Embedding Model Configuration
      - key: EMBEDDING_MODEL_NAME
        value: BAAI/bge-base-en-v1.5
      - key: EMBEDDING_DEVICE
        value: cpu
      - key: EMBEDDING_BATCH_SIZE
        value: 32

      # Qdrant Configuration
      - key: QDRANT_URL
        sync: false  # Set manually
      - key: QDRANT_API_KEY
        sync: false  # Set manually
      - key: QDRANT_COLLECTION_NAME
        value: book_content
      - key: QDRANT_VECTOR_SIZE
        value: 768

      # API Configuration
      - key: API_HOST
        value: 0.0.0.0
      - key: API_VERSION
        value: 1.0.0
      - key: ENVIRONMENT
        value: production

      # CORS - Update after deploying frontend
      - key: CORS_ORIGINS
        value: http://localhost:3000,https://muhammad-aneeq.github.io

      # Rate Limiting
      - key: RATE_LIMIT_PER_MINUTE
        value: 10

      # Logging
      - key: LOG_LEVEL
        value: INFO

      # Database URL - Using Neon Serverless PostgreSQL (set manually)
      - key: DATABASE_URL
        sync: false  # Set your Neon connection string in Render dashboard
